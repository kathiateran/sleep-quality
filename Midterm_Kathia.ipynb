{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1641</td>\n",
       "      <td>2.55250</td>\n",
       "      <td>1.08540</td>\n",
       "      <td>0.81841</td>\n",
       "      <td>1.92840</td>\n",
       "      <td>-1.358700</td>\n",
       "      <td>1.99830</td>\n",
       "      <td>2.22160</td>\n",
       "      <td>-0.56649</td>\n",
       "      <td>-1.797700</td>\n",
       "      <td>1.88500</td>\n",
       "      <td>1.92720</td>\n",
       "      <td>0.49176</td>\n",
       "      <td>1.79460</td>\n",
       "      <td>-1.183800</td>\n",
       "      <td>2.72480</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>-1.36850</td>\n",
       "      <td>-1.80890</td>\n",
       "      <td>-2.0512</td>\n",
       "      <td>-1.52230</td>\n",
       "      <td>1.878600</td>\n",
       "      <td>-1.61250</td>\n",
       "      <td>-1.42510</td>\n",
       "      <td>1.39340</td>\n",
       "      <td>1.402600</td>\n",
       "      <td>-1.71320</td>\n",
       "      <td>-1.52250</td>\n",
       "      <td>-1.07440</td>\n",
       "      <td>-1.66100</td>\n",
       "      <td>1.308900</td>\n",
       "      <td>-1.16060</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>-0.85805</td>\n",
       "      <td>-0.40671</td>\n",
       "      <td>-0.42681</td>\n",
       "      <td>-0.86298</td>\n",
       "      <td>1.317400</td>\n",
       "      <td>-1.01810</td>\n",
       "      <td>-0.82923</td>\n",
       "      <td>0.34643</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>-0.70581</td>\n",
       "      <td>-0.86439</td>\n",
       "      <td>-0.10365</td>\n",
       "      <td>-1.11450</td>\n",
       "      <td>-0.225820</td>\n",
       "      <td>-0.86877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2392</td>\n",
       "      <td>0.44826</td>\n",
       "      <td>0.93003</td>\n",
       "      <td>0.92964</td>\n",
       "      <td>0.47365</td>\n",
       "      <td>-0.325770</td>\n",
       "      <td>0.70826</td>\n",
       "      <td>0.43747</td>\n",
       "      <td>-0.28537</td>\n",
       "      <td>0.532420</td>\n",
       "      <td>0.52847</td>\n",
       "      <td>0.47346</td>\n",
       "      <td>1.25890</td>\n",
       "      <td>0.62846</td>\n",
       "      <td>-0.144060</td>\n",
       "      <td>0.36817</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>-0.59578</td>\n",
       "      <td>-0.14771</td>\n",
       "      <td>-0.18656</td>\n",
       "      <td>-0.43314</td>\n",
       "      <td>0.395580</td>\n",
       "      <td>-0.42397</td>\n",
       "      <td>-0.54890</td>\n",
       "      <td>0.31039</td>\n",
       "      <td>1.244100</td>\n",
       "      <td>-0.23077</td>\n",
       "      <td>-0.43503</td>\n",
       "      <td>0.10992</td>\n",
       "      <td>-0.47825</td>\n",
       "      <td>0.406930</td>\n",
       "      <td>-0.66396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>-1.35230</td>\n",
       "      <td>-1.49170</td>\n",
       "      <td>-2.4736</td>\n",
       "      <td>-1.47010</td>\n",
       "      <td>0.304800</td>\n",
       "      <td>-1.66660</td>\n",
       "      <td>-1.37570</td>\n",
       "      <td>1.41870</td>\n",
       "      <td>1.474500</td>\n",
       "      <td>-1.43200</td>\n",
       "      <td>-1.47020</td>\n",
       "      <td>-1.47790</td>\n",
       "      <td>-1.73380</td>\n",
       "      <td>0.311300</td>\n",
       "      <td>-1.15710</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>-0.68205</td>\n",
       "      <td>-0.63552</td>\n",
       "      <td>-0.63703</td>\n",
       "      <td>-0.46330</td>\n",
       "      <td>0.804410</td>\n",
       "      <td>-0.49418</td>\n",
       "      <td>-0.68431</td>\n",
       "      <td>0.26537</td>\n",
       "      <td>0.758440</td>\n",
       "      <td>-0.48570</td>\n",
       "      <td>-0.46347</td>\n",
       "      <td>-0.22501</td>\n",
       "      <td>-0.14301</td>\n",
       "      <td>0.819740</td>\n",
       "      <td>-0.62544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>-1.11670</td>\n",
       "      <td>-1.46730</td>\n",
       "      <td>-2.0288</td>\n",
       "      <td>-1.18480</td>\n",
       "      <td>1.510600</td>\n",
       "      <td>-1.32770</td>\n",
       "      <td>-1.13530</td>\n",
       "      <td>1.16150</td>\n",
       "      <td>1.318500</td>\n",
       "      <td>-1.08500</td>\n",
       "      <td>-1.18470</td>\n",
       "      <td>-0.88122</td>\n",
       "      <td>-1.36410</td>\n",
       "      <td>0.248890</td>\n",
       "      <td>-1.01970</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-1.18220</td>\n",
       "      <td>-1.65260</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>-1.31410</td>\n",
       "      <td>2.489300</td>\n",
       "      <td>-1.36300</td>\n",
       "      <td>-1.27680</td>\n",
       "      <td>0.85210</td>\n",
       "      <td>1.362200</td>\n",
       "      <td>-1.36560</td>\n",
       "      <td>-1.31390</td>\n",
       "      <td>-1.17200</td>\n",
       "      <td>-1.42210</td>\n",
       "      <td>1.275600</td>\n",
       "      <td>-1.06570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1753</td>\n",
       "      <td>0.45929</td>\n",
       "      <td>0.74507</td>\n",
       "      <td>0.49539</td>\n",
       "      <td>0.49519</td>\n",
       "      <td>-0.152260</td>\n",
       "      <td>0.84686</td>\n",
       "      <td>0.61644</td>\n",
       "      <td>-0.26968</td>\n",
       "      <td>-0.244540</td>\n",
       "      <td>0.72739</td>\n",
       "      <td>0.49574</td>\n",
       "      <td>-0.50034</td>\n",
       "      <td>0.71185</td>\n",
       "      <td>-0.300380</td>\n",
       "      <td>0.30305</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>1.29760</td>\n",
       "      <td>0.89921</td>\n",
       "      <td>0.6174</td>\n",
       "      <td>1.38020</td>\n",
       "      <td>-0.949390</td>\n",
       "      <td>1.33240</td>\n",
       "      <td>1.37700</td>\n",
       "      <td>-0.66845</td>\n",
       "      <td>-0.902800</td>\n",
       "      <td>1.50940</td>\n",
       "      <td>1.37910</td>\n",
       "      <td>-0.12021</td>\n",
       "      <td>1.15240</td>\n",
       "      <td>-0.896940</td>\n",
       "      <td>1.28540</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2399</td>\n",
       "      <td>0.17600</td>\n",
       "      <td>0.29763</td>\n",
       "      <td>0.51186</td>\n",
       "      <td>0.25393</td>\n",
       "      <td>0.029445</td>\n",
       "      <td>0.91027</td>\n",
       "      <td>0.12429</td>\n",
       "      <td>0.74095</td>\n",
       "      <td>0.512790</td>\n",
       "      <td>0.46707</td>\n",
       "      <td>0.25392</td>\n",
       "      <td>-0.11743</td>\n",
       "      <td>1.01890</td>\n",
       "      <td>1.040400</td>\n",
       "      <td>0.23183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>1.14130</td>\n",
       "      <td>0.88585</td>\n",
       "      <td>1.1497</td>\n",
       "      <td>0.98416</td>\n",
       "      <td>-0.681470</td>\n",
       "      <td>1.53900</td>\n",
       "      <td>0.96871</td>\n",
       "      <td>-1.36380</td>\n",
       "      <td>-0.950890</td>\n",
       "      <td>1.34210</td>\n",
       "      <td>0.98484</td>\n",
       "      <td>2.19650</td>\n",
       "      <td>1.63820</td>\n",
       "      <td>0.012372</td>\n",
       "      <td>1.48460</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2266</td>\n",
       "      <td>0.82336</td>\n",
       "      <td>0.96212</td>\n",
       "      <td>0.89724</td>\n",
       "      <td>0.72389</td>\n",
       "      <td>-0.745010</td>\n",
       "      <td>0.85034</td>\n",
       "      <td>0.79426</td>\n",
       "      <td>-0.56215</td>\n",
       "      <td>-0.327030</td>\n",
       "      <td>0.94201</td>\n",
       "      <td>0.72452</td>\n",
       "      <td>0.74694</td>\n",
       "      <td>0.63866</td>\n",
       "      <td>-0.627310</td>\n",
       "      <td>0.60922</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>-1.39830</td>\n",
       "      <td>-1.94680</td>\n",
       "      <td>-1.8159</td>\n",
       "      <td>-1.61780</td>\n",
       "      <td>1.117000</td>\n",
       "      <td>-1.78410</td>\n",
       "      <td>-1.49280</td>\n",
       "      <td>0.47377</td>\n",
       "      <td>1.389300</td>\n",
       "      <td>-1.69350</td>\n",
       "      <td>-1.61780</td>\n",
       "      <td>-1.35640</td>\n",
       "      <td>-1.86380</td>\n",
       "      <td>0.802690</td>\n",
       "      <td>-1.17550</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1603</td>\n",
       "      <td>1.14840</td>\n",
       "      <td>0.96882</td>\n",
       "      <td>-0.059307</td>\n",
       "      <td>1.05240</td>\n",
       "      <td>-0.995970</td>\n",
       "      <td>1.40450</td>\n",
       "      <td>1.10750</td>\n",
       "      <td>-0.50186</td>\n",
       "      <td>-0.616550</td>\n",
       "      <td>0.62189</td>\n",
       "      <td>1.05310</td>\n",
       "      <td>-1.31880</td>\n",
       "      <td>1.38270</td>\n",
       "      <td>-0.406800</td>\n",
       "      <td>1.16570</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2345</td>\n",
       "      <td>1.45410</td>\n",
       "      <td>1.11970</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>0.89145</td>\n",
       "      <td>-1.078500</td>\n",
       "      <td>1.31970</td>\n",
       "      <td>1.13800</td>\n",
       "      <td>0.20823</td>\n",
       "      <td>-1.454500</td>\n",
       "      <td>0.75553</td>\n",
       "      <td>0.89093</td>\n",
       "      <td>-1.00910</td>\n",
       "      <td>1.07750</td>\n",
       "      <td>-0.563950</td>\n",
       "      <td>1.03100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>-1.35630</td>\n",
       "      <td>-1.46790</td>\n",
       "      <td>-1.6943</td>\n",
       "      <td>-1.46640</td>\n",
       "      <td>1.465500</td>\n",
       "      <td>-1.64060</td>\n",
       "      <td>-1.37470</td>\n",
       "      <td>0.68590</td>\n",
       "      <td>0.683230</td>\n",
       "      <td>-1.57070</td>\n",
       "      <td>-1.46610</td>\n",
       "      <td>-1.41880</td>\n",
       "      <td>-1.68550</td>\n",
       "      <td>0.487640</td>\n",
       "      <td>-1.15860</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.28214</td>\n",
       "      <td>-2.29790</td>\n",
       "      <td>-1.0484</td>\n",
       "      <td>1.52950</td>\n",
       "      <td>-1.013500</td>\n",
       "      <td>0.81719</td>\n",
       "      <td>1.01240</td>\n",
       "      <td>-0.81219</td>\n",
       "      <td>-2.354000</td>\n",
       "      <td>2.89310</td>\n",
       "      <td>1.52970</td>\n",
       "      <td>1.61860</td>\n",
       "      <td>0.60676</td>\n",
       "      <td>-1.023700</td>\n",
       "      <td>0.68563</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>-0.60633</td>\n",
       "      <td>-0.20169</td>\n",
       "      <td>-0.074485</td>\n",
       "      <td>-0.40404</td>\n",
       "      <td>-0.334420</td>\n",
       "      <td>-0.57903</td>\n",
       "      <td>-0.59231</td>\n",
       "      <td>0.66574</td>\n",
       "      <td>-0.037666</td>\n",
       "      <td>-0.32603</td>\n",
       "      <td>-0.40403</td>\n",
       "      <td>0.74849</td>\n",
       "      <td>-0.69024</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>-0.65907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat1    feat2      feat3    feat4     feat5    feat6    feat7  \\\n",
       "1641  2.55250  1.08540    0.81841  1.92840 -1.358700  1.99830  2.22160   \n",
       "1026 -1.36850 -1.80890    -2.0512 -1.52230  1.878600 -1.61250 -1.42510   \n",
       "706  -0.85805 -0.40671   -0.42681 -0.86298  1.317400 -1.01810 -0.82923   \n",
       "2392  0.44826  0.93003    0.92964  0.47365 -0.325770  0.70826  0.43747   \n",
       "424  -0.59578 -0.14771   -0.18656 -0.43314  0.395580 -0.42397 -0.54890   \n",
       "1535 -1.35230 -1.49170    -2.4736 -1.47010  0.304800 -1.66660 -1.37570   \n",
       "378  -0.68205 -0.63552   -0.63703 -0.46330  0.804410 -0.49418 -0.68431   \n",
       "1277 -1.11670 -1.46730    -2.0288 -1.18480  1.510600 -1.32770 -1.13530   \n",
       "200  -1.18220 -1.65260     -1.995 -1.31410  2.489300 -1.36300 -1.27680   \n",
       "1753  0.45929  0.74507    0.49539  0.49519 -0.152260  0.84686  0.61644   \n",
       "1616  1.29760  0.89921     0.6174  1.38020 -0.949390  1.33240  1.37700   \n",
       "2399  0.17600  0.29763    0.51186  0.25393  0.029445  0.91027  0.12429   \n",
       "1833  1.14130  0.88585     1.1497  0.98416 -0.681470  1.53900  0.96871   \n",
       "2266  0.82336  0.96212    0.89724  0.72389 -0.745010  0.85034  0.79426   \n",
       "1328 -1.39830 -1.94680    -1.8159 -1.61780  1.117000 -1.78410 -1.49280   \n",
       "1603  1.14840  0.96882  -0.059307  1.05240 -0.995970  1.40450  1.10750   \n",
       "2345  1.45410  1.11970     0.1161  0.89145 -1.078500  1.31970  1.13800   \n",
       "884  -1.35630 -1.46790    -1.6943 -1.46640  1.465500 -1.64060 -1.37470   \n",
       "1770  0.28214 -2.29790    -1.0484  1.52950 -1.013500  0.81719  1.01240   \n",
       "155  -0.60633 -0.20169  -0.074485 -0.40404 -0.334420 -0.57903 -0.59231   \n",
       "\n",
       "        feat8     feat9   feat10   feat11   feat12   feat13    feat14  \\\n",
       "1641 -0.56649 -1.797700  1.88500  1.92720  0.49176  1.79460 -1.183800   \n",
       "1026  1.39340  1.402600 -1.71320 -1.52250 -1.07440 -1.66100  1.308900   \n",
       "706   0.34643  0.946580 -0.70581 -0.86439 -0.10365 -1.11450 -0.225820   \n",
       "2392 -0.28537  0.532420  0.52847  0.47346  1.25890  0.62846 -0.144060   \n",
       "424   0.31039  1.244100 -0.23077 -0.43503  0.10992 -0.47825  0.406930   \n",
       "1535  1.41870  1.474500 -1.43200 -1.47020 -1.47790 -1.73380  0.311300   \n",
       "378   0.26537  0.758440 -0.48570 -0.46347 -0.22501 -0.14301  0.819740   \n",
       "1277  1.16150  1.318500 -1.08500 -1.18470 -0.88122 -1.36410  0.248890   \n",
       "200   0.85210  1.362200 -1.36560 -1.31390 -1.17200 -1.42210  1.275600   \n",
       "1753 -0.26968 -0.244540  0.72739  0.49574 -0.50034  0.71185 -0.300380   \n",
       "1616 -0.66845 -0.902800  1.50940  1.37910 -0.12021  1.15240 -0.896940   \n",
       "2399  0.74095  0.512790  0.46707  0.25392 -0.11743  1.01890  1.040400   \n",
       "1833 -1.36380 -0.950890  1.34210  0.98484  2.19650  1.63820  0.012372   \n",
       "2266 -0.56215 -0.327030  0.94201  0.72452  0.74694  0.63866 -0.627310   \n",
       "1328  0.47377  1.389300 -1.69350 -1.61780 -1.35640 -1.86380  0.802690   \n",
       "1603 -0.50186 -0.616550  0.62189  1.05310 -1.31880  1.38270 -0.406800   \n",
       "2345  0.20823 -1.454500  0.75553  0.89093 -1.00910  1.07750 -0.563950   \n",
       "884   0.68590  0.683230 -1.57070 -1.46610 -1.41880 -1.68550  0.487640   \n",
       "1770 -0.81219 -2.354000  2.89310  1.52970  1.61860  0.60676 -1.023700   \n",
       "155   0.66574 -0.037666 -0.32603 -0.40403  0.74849 -0.69024  0.141800   \n",
       "\n",
       "       feat15  label  \n",
       "1641  2.72480      3  \n",
       "1026 -1.16060      2  \n",
       "706  -0.86877      1  \n",
       "2392  0.36817      3  \n",
       "424  -0.66396      1  \n",
       "1535 -1.15710      2  \n",
       "378  -0.62544      1  \n",
       "1277 -1.01970      2  \n",
       "200  -1.06570      1  \n",
       "1753  0.30305      3  \n",
       "1616  1.28540      3  \n",
       "2399  0.23183      3  \n",
       "1833  1.48460      3  \n",
       "2266  0.60922      3  \n",
       "1328 -1.17550      2  \n",
       "1603  1.16570      3  \n",
       "2345  1.03100      3  \n",
       "884  -1.15860      2  \n",
       "1770  0.68563      3  \n",
       "155  -0.65907      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# load dataset\n",
    "names = ['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6', 'feat7', 'feat8', 'feat9', 'feat10', 'feat11', 'feat12', 'feat13', 'feat14', 'feat15', 'label']\n",
    "dataframe = pd.read_csv('/Users/kathiateran/Documents/Machine Learning/kathia_teran_midterm/sleep_quality_sc-3.csv', names=names)\n",
    "array = dataframe.values\n",
    "dataframe.sample(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mClassifier Performance with 1 Feature \n",
      "\u001b[0m\n",
      "\u001b[91mMultilayer Perceptron Accuracy: 0.848333\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.78      0.78       800\n",
      "           2       0.84      0.89      0.87       800\n",
      "           3       0.93      0.89      0.91       800\n",
      "\n",
      "    accuracy                           0.85      2400\n",
      "   macro avg       0.85      0.85      0.85      2400\n",
      "weighted avg       0.85      0.85      0.85      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mLinear Discriminant Analysis Accuracy: 0.740417\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.61      0.61       800\n",
      "           2       0.68      0.81      0.74       800\n",
      "           3       0.98      0.81      0.88       800\n",
      "\n",
      "    accuracy                           0.74      2400\n",
      "   macro avg       0.76      0.74      0.74      2400\n",
      "weighted avg       0.76      0.74      0.74      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mDecision Tree Accuracy: 0.793333\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.68      0.70       800\n",
      "           2       0.81      0.83      0.82       800\n",
      "           3       0.85      0.87      0.86       800\n",
      "\n",
      "    accuracy                           0.79      2400\n",
      "   macro avg       0.79      0.79      0.79      2400\n",
      "weighted avg       0.79      0.79      0.79      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mNaive Bayes Accuracy: 0.855833\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.78      0.79       800\n",
      "           2       0.84      0.91      0.87       800\n",
      "           3       0.95      0.87      0.91       800\n",
      "\n",
      "    accuracy                           0.86      2400\n",
      "   macro avg       0.86      0.86      0.86      2400\n",
      "weighted avg       0.86      0.86      0.86      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[1mClassifier Performance with 2 Features \n",
      "\u001b[0m\n",
      "\u001b[91mMultilayer Perceptron Accuracy: 0.885833\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.86      0.85       800\n",
      "           2       0.89      0.89      0.89       800\n",
      "           3       0.97      0.95      0.96       800\n",
      "\n",
      "    accuracy                           0.90      2400\n",
      "   macro avg       0.90      0.90      0.90      2400\n",
      "weighted avg       0.90      0.90      0.90      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mLinear Discriminant Analysis Accuracy: 0.820833\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.75      0.74       800\n",
      "           2       0.77      0.84      0.80       800\n",
      "           3       0.98      0.87      0.92       800\n",
      "\n",
      "    accuracy                           0.82      2400\n",
      "   macro avg       0.83      0.82      0.82      2400\n",
      "weighted avg       0.83      0.82      0.82      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mDecision Tree Accuracy: 0.862083\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.78      0.80       800\n",
      "           2       0.83      0.86      0.85       800\n",
      "           3       0.95      0.95      0.95       800\n",
      "\n",
      "    accuracy                           0.86      2400\n",
      "   macro avg       0.86      0.86      0.86      2400\n",
      "weighted avg       0.86      0.86      0.86      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[91mNaive Bayes Accuracy: 0.887083\u001b[0m\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.83      0.83       800\n",
      "           2       0.87      0.90      0.89       800\n",
      "           3       0.96      0.92      0.94       800\n",
      "\n",
      "    accuracy                           0.89      2400\n",
      "   macro avg       0.89      0.89      0.89      2400\n",
      "weighted avg       0.89      0.89      0.89      2400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy [0.88916667 0.92291667 0.96208333]\n",
      "True Positive Rate [0.83375 0.90375 0.92375]\n",
      "Specificity [0.916875 0.9325   0.98125 ]\n",
      "False Positive Rate [0.083125 0.0675   0.01875 ]\n",
      "False Negative Rate [0.16625 0.09625 0.07625]\n",
      "\n",
      "\n",
      "Average Accuracy 92.47%\n",
      "Average True Positive Rate 88.71%\n",
      "Average Specificity 94.35%\n",
      "Average False Positive Rate 5.65%\n",
      "Average False Negative Rate 11.29%\n",
      "\n",
      "\n",
      "\u001b[1mClassifier Performance with 3 Features \n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '\"7.4637e-05\"\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e44601b07177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \"\"\"\n\u001b[1;32m    981\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 982\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    321\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 919\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '\"7.4637e-05\"\\''"
     ]
    }
   ],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "for n in range(1,15):\n",
    "    X = array[:,0:n]\n",
    "    Y = array[:,15]\n",
    "    \n",
    "    Y=Y.astype('int') \n",
    "    # prepare configuration for cross validation test harness\n",
    "    seed = 7\n",
    "    \n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('Multilayer Perceptron', MLPClassifier(hidden_layer_sizes=(30, 30, 30), max_iter=800)))\n",
    "    models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
    "    models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "    models.append(('Naive Bayes', GaussianNB()))\n",
    "\n",
    "    numfeatures = str(n)\n",
    "    \n",
    "    if (n == 1):\n",
    "        print('\\033[1m' + \"Classifier Performance with \" + numfeatures + \" Feature \\n\" + '\\033[0m')\n",
    "    \n",
    "    else:\n",
    "        print('\\033[1m' + \"Classifier Performance with \" + numfeatures + \" Features \\n\" + '\\033[0m')\n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'accuracy'\n",
    "   \n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "        y_pred = cross_val_predict(model, X, Y, cv=kfold)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s Accuracy: %f\" % (name, cv_results.mean())\n",
    "        print('\\033[91m' + msg + '\\033[0m')\n",
    "        \n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"Classification Report:\\n\\n\",classification_report(Y,y_pred))\n",
    "        ####\n",
    "        #cm = confusion_matrix(Y, y_pred)\n",
    "        #df = pd.DataFrame(cm, columns=['Predicted Class 1', 'Predicted Class 2', 'Predicted Class 3'], \n",
    "         #                 index=['True Class 1', 'True Class 2', 'True Class 3'])\n",
    "        #print(df)\n",
    "        \n",
    "        ###\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP) \n",
    "        FPR = FP/(FP+TN)\n",
    "        FNR = FN/(TP+FN)\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        \n",
    "        print(\"Accuracy\", ACC)\n",
    "        print(\"True Positive Rate\", TPR)\n",
    "        print(\"Specificity\", TNR)\n",
    "        print(\"False Positive Rate\", FPR)\n",
    "        print(\"False Negative Rate\", FNR) \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        ACCavg = np.mean(ACC)\n",
    "        TPRavg = np.mean(TPR)\n",
    "        TNRavg = np.mean(TNR)\n",
    "        FPRavg = np.mean(FPR)\n",
    "        FNRavg = np.mean(FNR)\n",
    "            \n",
    "        print(\"Average Accuracy\", '{:.2%}'.format(ACCavg))\n",
    "        print(\"Average True Positive Rate\", '{:.2%}'.format(TPRavg))\n",
    "        print(\"Average Specificity\", '{:.2%}'.format(TNRavg))\n",
    "        print(\"Average False Positive Rate\", '{:.2%}'.format(FPRavg))\n",
    "        print(\"Average False Negative Rate\", '{:.2%}'.format(FNRavg))\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
